{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abiola0101/Go-Data/blob/main/GoData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZXNy8AwKKRd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZxYJc2Km6j2"
      },
      "outputs": [],
      "source": [
        "# import necessary python libraries to visualize and manipulate the data\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZljh6s_KM7C"
      },
      "outputs": [],
      "source": [
        "#import data file into a pandas dataframe\n",
        "df_goData = pd.read_csv('CBB_Listings.csv', on_bad_lines='skip')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVRpecxDISbe"
      },
      "source": [
        "**1. Exploring CBB_Listing Dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYO42Fs0IRVn"
      },
      "outputs": [],
      "source": [
        "# display the first 5 rows\n",
        "df_goData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt81u1XOInPa"
      },
      "outputs": [],
      "source": [
        "# display the last 5 rows\n",
        "df_goData.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMNKqgb32pGL"
      },
      "outputs": [],
      "source": [
        "# making a list of the columns in the dataset\n",
        "df_goData.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xK3biSFLbVO"
      },
      "outputs": [],
      "source": [
        "# get general information about the dataset\n",
        "df_goData.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25OoSjSyMAdc"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. There are 145144 rows and 46 columns\n",
        "2. 9 columns contain null values:\n",
        "\n",
        "  listing_heading,\n",
        "  dealer_email,\n",
        "  dealer_phone,\n",
        "  series,\n",
        "  exterior_color,\n",
        "  exterior_color_category,\n",
        "  interior_color,\n",
        "  interior_color_category,\n",
        "  listing_dropoff_date\n",
        "\n",
        "3. The entire column of dealer email has no data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2BLXIi4LufG"
      },
      "outputs": [],
      "source": [
        "# get the number of rows and columns in the dataset\n",
        "\n",
        "df_goData.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZUvAw_bE4kR"
      },
      "outputs": [],
      "source": [
        "# checking the data type for each column\n",
        "df_goData.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3LlENd9Geim"
      },
      "outputs": [],
      "source": [
        "# checking for unique values in each column\n",
        "df_goData.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiCqQp033Fnl"
      },
      "outputs": [],
      "source": [
        "# checking for duplicates\n",
        "\n",
        "df_goData.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUuCmg7EmV2g"
      },
      "outputs": [],
      "source": [
        "# generating correlation matrix\n",
        "corr_matrix = df_goData.corr(numeric_only = True)\n",
        "corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tR44gqxmiGd"
      },
      "outputs": [],
      "source": [
        "# Creating a heatmap using Seaborn\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt= \".2f\")\n",
        "plt.title('Correlation Heatmap', size = 24)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5-1RASH_1m2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmWBAJJgRPfK"
      },
      "outputs": [],
      "source": [
        "pip install ydata-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA8Zu0zsMU-5"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcccJUbU6lf-"
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(df_goData, title=\"Pandas Profiling Report\", explorative=True)\n",
        "profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsuhITSwZol4"
      },
      "source": [
        "### **2a. Data cleaning: Removing null values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inL1ROTBV86a"
      },
      "outputs": [],
      "source": [
        "# enabling copy on write to avoid creating unnecessary copies\n",
        "pd.options.mode.copy_on_write = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQHEE7AIRfYx"
      },
      "outputs": [],
      "source": [
        "# filling null values in series with unknown\n",
        "df_goData.fillna('unknown', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3j5lijVSMnk"
      },
      "outputs": [],
      "source": [
        "# confirming there are no cells containing null in the dataframe\n",
        "df_goData.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tlJtYgTTE4N"
      },
      "outputs": [],
      "source": [
        "# confirming there are no cells containing null in the dataframe\n",
        "df_goData.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h4xwijkaSJf"
      },
      "source": [
        "### **2b. Data cleaning: Removing Duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWBYWEQIbBX8"
      },
      "outputs": [],
      "source": [
        "# checking for duplicate rows\n",
        "duplicate_rows = df_goData.duplicated()\n",
        "duplicate_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZlzGDDJbJBq"
      },
      "outputs": [],
      "source": [
        "duplicate_rows.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp0nBsIvcbgZ"
      },
      "source": [
        "**Since there is only 1 unique entry in duplicate rows, it means that there are no duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39vutawjdErN"
      },
      "outputs": [],
      "source": [
        "df_goData.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zEXadxfxlL-"
      },
      "source": [
        "### **2c. Data cleaning: Removing Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTM5-Vo7M-1W"
      },
      "source": [
        "**Removing outliers from price coulum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkp0xXqAx1WC"
      },
      "outputs": [],
      "source": [
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = df_goData['price'].quantile(0.25)\n",
        "Q3 = df_goData['price'].quantile(0.75)\n",
        "\n",
        "# Calculate IQR\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Remove outliers\n",
        "df_no_outliers = df_goData[(df_goData['price'] >= lower_bound) & (df_goData['price'] <= upper_bound)]\n",
        "\n",
        "#print(\"Original Data:\")\n",
        "#print(df_clean)\n",
        "#print(\"\\nData without outliers:\")\n",
        "#print(df_no_outliers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8NN57LzoCrW"
      },
      "outputs": [],
      "source": [
        "df_no_outliers.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRP-Ps8GNPmK"
      },
      "source": [
        "**Removing outliers from mileage column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMFumme28Hd_"
      },
      "outputs": [],
      "source": [
        "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "Q1 = df_goData['mileage'].quantile(0.25)\n",
        "Q3 = df_goData['mileage'].quantile(0.75)\n",
        "\n",
        "# Calculate IQR\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Remove outliers\n",
        "df_no_outliers = df_goData[(df_goData['mileage'] >= lower_bound) & (df_goData['mileage'] <= upper_bound)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvW83maC8YQU"
      },
      "outputs": [],
      "source": [
        "df_no_outliers.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mSxXkcqNAha"
      },
      "outputs": [],
      "source": [
        "df_no_outliers.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOjgMKRremhN"
      },
      "outputs": [],
      "source": [
        "df_no_outliers.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qBTrqLxezL_"
      },
      "source": [
        "## **2d. Data cleaning: Assigning 6 and 7 in transmission_from_vin column to manual and automatic respectively**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFRCutS3hL3b"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries for transmission_from_vin column\n",
        "df_no_outliers['transmission_from_vin'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUJH__VNhzk4"
      },
      "outputs": [],
      "source": [
        "df_no_outliers.replace({'transmission_from_vin': '6'}, 'M', inplace=True)\n",
        "df_no_outliers.replace({'transmission_from_vin': '7'}, 'A', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz0kBuahhdOh"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries for transmission_from_vin column after replacing 6 and 7\n",
        "df_no_outliers['transmission_from_vin'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp0a1Yh8H2ti"
      },
      "source": [
        "### **3. Identifying Significant Attributes for Problem 3.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slPI2SXbir5f"
      },
      "source": [
        "Based on our research into car features, we identified 18 features that has a high potential to make accurate predictions on vehicle transmission type. Following this selction, we are using Chi-square technique to identify features (from these 18) that would best make good predictions, there by reducing the number of features from 18 initially selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKfwtjucyRWM"
      },
      "outputs": [],
      "source": [
        "# creating a new dataframe containing relevant features\n",
        "df_features = df_no_outliers[['model_year', 'make', 'model', 'mileage', 'price', 'series', 'style', 'dealer_type', 'stock_type', 'days_on_market', 'certified', 'vin',\n",
        "                         'drivetrain_from_vin', 'engine_from_vin', 'wheelbase_from_vin','fuel_type_from_vin', 'number_price_changes','transmission_from_vin']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0XY4y3Aaage"
      },
      "outputs": [],
      "source": [
        "df_features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P2IinqrkFx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # Import OrdinalEncoder\n",
        "\n",
        "X = df_features.drop('transmission_from_vin', axis=1)  # Features\n",
        "y = df_features['transmission_from_vin']  # Target\n",
        "\n",
        "# Convert categorical features to numerical using OrdinalEncoder\n",
        "encoder = OrdinalEncoder() # Initialize OrdinalEncoder\n",
        "X_encoded = encoder.fit_transform(X) # Fit and transform X\n",
        "\n",
        "# Chi-squared test\n",
        "chi_scores = chi2(X_encoded, y) # Use encoded X for chi2 test\n",
        "p_values = pd.Series(chi_scores[1], index=X.columns)\n",
        "p_values.sort_values(ascending=True, inplace=True)\n",
        "print(p_values)  # Features with lower p-values are more important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLFE3wuhzmoZ"
      },
      "source": [
        "From the result of Chi-Square test, **model_year, model, number_price_changes, stock_type, dealer_type, fuel_type_from_vin, and certified** have the lowest p-values and are the most useful in making accurate predictions. In addition to these 7, we will include **make, mileage and price** which we have been instructed to include as features in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbHThAL8IDRG"
      },
      "outputs": [],
      "source": [
        "# creating a new dataframe containing relevant 11 features\n",
        "df_model_features = df_features[['model_year', 'make', 'model', 'mileage', 'price', 'number_price_changes',\n",
        "                              'stock_type', 'dealer_type', 'fuel_type_from_vin', 'certified', 'transmission_from_vin']]\n",
        "\n",
        "# displaying the new dataframe\n",
        "df_model_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg0abRwT1SIK"
      },
      "outputs": [],
      "source": [
        "df_model_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSU7kePs2gIm"
      },
      "outputs": [],
      "source": [
        "df_model_features.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpiL3rvycNcL"
      },
      "source": [
        "###**4. Splitting data into Train and Test sets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df088vwRcOff"
      },
      "outputs": [],
      "source": [
        "#importing train_test_split library\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwo6ka38ds2m"
      },
      "outputs": [],
      "source": [
        "# defining the independent (X) and dependent (y) variables\n",
        "X = df_model_features.drop('transmission_from_vin', axis=1)\n",
        "y = df_model_features['transmission_from_vin']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t59Ct5wRdp2F"
      },
      "outputs": [],
      "source": [
        "# splitting the dataset into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.65, stratify=y, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkOObNJBdJqQ"
      },
      "outputs": [],
      "source": [
        "# validating the shape of the train and test sets\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1VH9gtJ5MOk"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r3Bv8WXjJhl"
      },
      "outputs": [],
      "source": [
        "X_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9mbM2bH7U9K"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT2k-hcBjPZ8"
      },
      "outputs": [],
      "source": [
        "X_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pax7AQ4n7XwQ"
      },
      "outputs": [],
      "source": [
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxd093Na7azp"
      },
      "outputs": [],
      "source": [
        "y_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M13W0teu2AWF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6DRSXNuxeNG"
      },
      "source": [
        "### **5. Data Pre-processing - Encoding Categorical columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z-S9O_ryck2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ak9Cudc9bzC"
      },
      "outputs": [],
      "source": [
        "pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o42oKrlh9ftt"
      },
      "outputs": [],
      "source": [
        "from category_encoders import BinaryEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFSKehmU0kDc"
      },
      "source": [
        "###**Encoding X_train**\n",
        "**i) Encoding the 'make' column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEoTdiSSzTTy"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries in 'make' column\n",
        "X_train['make'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOvncNxj1Jvk"
      },
      "outputs": [],
      "source": [
        "# checking the count of unique entries in 'make' column\n",
        "X_train['make'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxAeYIUctFZe"
      },
      "outputs": [],
      "source": [
        "# Encoding the 'make' column with BinaryEncoder\n",
        "be_make = BinaryEncoder(cols = ['make'])\n",
        "X_train = be_make.fit_transform(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGnsqSRQ40M4"
      },
      "outputs": [],
      "source": [
        "# confirming the 'make' column has been encoded\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob4K42td5koi"
      },
      "source": [
        "**ii)Encoding the 'model' column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHogfLnr5j7f"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries in 'model' column\n",
        "X_train['model'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PImfHE9K5j4m"
      },
      "outputs": [],
      "source": [
        "X_train['model'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8afs45KHDCVa"
      },
      "outputs": [],
      "source": [
        "# Encoding the 'model' column with BinaryEncoder\n",
        "be_model = BinaryEncoder(cols = ['model'])\n",
        "X_train = be_model.fit_transform(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtHIPejZDPxC"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSo8vCoO2ErX"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmLD-ToU7MgD"
      },
      "source": [
        "**iii) Encoding the 'stock_type' column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-8FOFll7LtC"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries in the 'stock_type' column\n",
        "X_train['stock_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6TtkCBJ7Ll9"
      },
      "outputs": [],
      "source": [
        "# Encoding the stock_type column with LabelEncoder\n",
        "\n",
        "le_number_stock_type = LabelEncoder()\n",
        "X_train['stock_type'] = le_number_stock_type.fit_transform(X_train['stock_type'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR596Mw67Lb_"
      },
      "outputs": [],
      "source": [
        "#checking 'exterior_color_category' column has been encoded and the 1631 unique entries have been captured in 11 columns\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBXeKX8YCxmo"
      },
      "source": [
        "**vii) Encoding the 'dealer_type' column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xziTsbrZ3-tY"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries in the 'dealer_type' column\n",
        "X_train['dealer_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a27e9ABXCJS5"
      },
      "outputs": [],
      "source": [
        "# Encoding the dealer_type column with LabelEncoder\n",
        "\n",
        "le_dealer_type = LabelEncoder()\n",
        "X_train['dealer_type'] = le_dealer_type.fit_transform(X_train['dealer_type'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zfDQKyj4Lut"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9R1rCRMClUk"
      },
      "source": [
        "**viii) Encoding the 'fuel_type_from_vin' column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa_TWqni5I5f"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries in the 'fuel_type_from_vin' column\n",
        "X_train['fuel_type_from_vin'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvMx_pszGv3I"
      },
      "outputs": [],
      "source": [
        "X_test['fuel_type_from_vin'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3UQCkLU6V73"
      },
      "outputs": [],
      "source": [
        "# Encoding the fuel_type_from_vin column with OnehotEncoder\n",
        "X_train = pd.get_dummies(X_train, columns=['fuel_type_from_vin'], dtype = 'int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oLwgcB15TnN"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soNWLVp7F1fJ"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrVgw6bQ_tl4"
      },
      "source": [
        "### **Encoding X_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT93n8GgADZn"
      },
      "outputs": [],
      "source": [
        "# checking the unique entries in 'make' column\n",
        "X_test['make'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSGoz3BtAPak"
      },
      "outputs": [],
      "source": [
        "X_test['make'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRvuhpT0AYZb"
      },
      "outputs": [],
      "source": [
        "# Encoding the 'make' column with BinaryEncoder\n",
        "be_make = BinaryEncoder(cols = ['make'])\n",
        "X_test = be_make.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JIHcmzvAl4L"
      },
      "outputs": [],
      "source": [
        "# Encoding the 'model' column with BinaryEncoder\n",
        "be_model = BinaryEncoder(cols = ['model'])\n",
        "X_test = be_model.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i86Un1lDA-Ay"
      },
      "outputs": [],
      "source": [
        "# Encoding the stock_type column with LabelEncoder\n",
        "\n",
        "le_number_stock_type = LabelEncoder()\n",
        "X_test['stock_type'] = le_number_stock_type.fit_transform(X_test['stock_type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp-rH7bIA3kB"
      },
      "outputs": [],
      "source": [
        "# Encoding the dealer_type column with LabelEncoder\n",
        "\n",
        "le_number_dealer_type = LabelEncoder()\n",
        "X_test['dealer_type'] = le_number_dealer_type.fit_transform(X_test['dealer_type'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtVn9_PGAyFm"
      },
      "outputs": [],
      "source": [
        "# Encoding the fuel_type_from_vin column with OnehotEncoder\n",
        "X_test = pd.get_dummies(X_test, columns=['fuel_type_from_vin'], dtype = 'int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRO_lkN9Jthi"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK20zv5bfrA1"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw1dOyP2fwCE"
      },
      "outputs": [],
      "source": [
        "X_test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cCceKS4TAH0"
      },
      "source": [
        "### **Encoding y_train & y_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqFL3jgJLtbv"
      },
      "outputs": [],
      "source": [
        "y_train = pd.get_dummies(y_train, columns=['transmission_from_vin'], dtype = 'int', drop_first= True)\n",
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6oomRSrlY4W"
      },
      "outputs": [],
      "source": [
        "y_test = pd.get_dummies(y_test, columns=['y_test'], dtype = 'int', drop_first= True)\n",
        "y_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxBzkBSwli_c"
      },
      "outputs": [],
      "source": [
        "# rename M to transmission_from_vin in y_train and y_test\n",
        "y_train.rename(columns={'M': 'transmission_from_vin'}, inplace=True)\n",
        "y_test.rename(columns={'M': 'transmission_from_vin'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgtEv-D4mHj1"
      },
      "outputs": [],
      "source": [
        "print(y_train.info())\n",
        "print(y_test.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTK27CvlmVvO"
      },
      "outputs": [],
      "source": [
        "print(y_train.head())\n",
        "print(y_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utu4TCqCcjP3"
      },
      "source": [
        "### **6. Handling Imbalanced data columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crbpvkFP7gCB"
      },
      "outputs": [],
      "source": [
        "pip install ydata-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEOUtpq-7nLC"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_59U5P-p7xji"
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(X_train, title=\"Pandas Profiling Report\", explorative=True)\n",
        "profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ55gi6sBZW7"
      },
      "source": [
        "The result of profiling X_train set after encoding shows high imbalance in\n",
        "- **model_0**\n",
        "- **model_1**\n",
        "-\t**certified**\n",
        "- **fuel_type_from_vin_CNG**\n",
        "-\t**fuel_type_from_vin_Diesel**\n",
        "-\t**fuel_type_from_vin_Electric**\n",
        "-\t**fuel_type_from_vin_Hybrid**\n",
        "-\t**fuel_type_from_vin_Hydrogen**\n",
        "- **fuel_type_from_vin_PHEV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8bktNnbBbbs"
      },
      "outputs": [],
      "source": [
        "#count the number of classes in each imbalanced column in the Train set\n",
        "print(X_train['model_0'].value_counts())\n",
        "print(X_train['model_1'].value_counts())\n",
        "print(X_train['certified'].value_counts())\n",
        "print(X_train['fuel_type_from_vin_CNG'].value_counts())\n",
        "print(X_train['fuel_type_from_vin_Diesel'].value_counts())\n",
        "print(X_train['fuel_type_from_vin_Electric'].value_counts())\n",
        "print(X_train['fuel_type_from_vin_Hybrid'].value_counts())\n",
        "print(X_train['fuel_type_from_vin_Hydrogen'].value_counts())\n",
        "print(X_train['fuel_type_from_vin_PHEV'].value_counts())\n",
        "print(y_train['transmission_from_vin'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpb60Ll-9_0E"
      },
      "outputs": [],
      "source": [
        "#count the number of classes in each imbalanced column in the Test set\n",
        "print(X_test['model_0'].value_counts())\n",
        "print(X_test['model_1'].value_counts())\n",
        "print(X_test['certified'].value_counts())\n",
        "print(X_test['fuel_type_from_vin_CNG'].value_counts())\n",
        "print(X_test['fuel_type_from_vin_Diesel'].value_counts())\n",
        "print(X_test['fuel_type_from_vin_Electric'].value_counts())\n",
        "print(X_test['fuel_type_from_vin_Hybrid'].value_counts())\n",
        "print(X_test['fuel_type_from_vin_Hydrogen'].value_counts())\n",
        "print(X_test['fuel_type_from_vin_PHEV'].value_counts())\n",
        "print(y_test['transmission_from_vin'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUXdazXYcmoG"
      },
      "source": [
        "**Visualization of Imbalanced columns in the Train set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VneKdkRjFUr-"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create individual countplots for each column\n",
        "fig, axes = plt.subplots(5, 2, figsize=(15, 20))  # Adjust figsize as needed\n",
        "\n",
        "sns.countplot(x='model_0', hue = 'model_0', data=X_train, ax=axes[0, 0])\n",
        "sns.countplot(x='model_1', hue = 'model_1', data=X_train, ax=axes[0, 1])\n",
        "sns.countplot(x='certified', hue = 'certified', data=X_train, ax=axes[1, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_CNG', hue = 'fuel_type_from_vin_CNG', data=X_train, ax=axes[1, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Diesel', hue = 'fuel_type_from_vin_Diesel', data=X_train, ax=axes[2, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Electric', hue = 'fuel_type_from_vin_Electric', data=X_train, ax=axes[2, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Hybrid', hue = 'fuel_type_from_vin_Hybrid', data=X_train, ax=axes[3, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Hydrogen', hue = 'fuel_type_from_vin_Hydrogen', data=X_train, ax=axes[3, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_PHEV', hue = 'fuel_type_from_vin_PHEV', data=X_train, ax=axes[4, 0])\n",
        "sns.countplot(x='transmission_from_vin', hue = 'transmission_from_vin', data=y_train, ax=axes[4, 1])\n",
        "# clear extra subplots to avoid empty plots\n",
        "# axes[4, 1].axis('off')\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN40rCnYcmj2"
      },
      "source": [
        "**Using SMOTE technique to handle imbalance in Train set**"
      ]
    },
    {
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Select specific columns (features to balance)\n",
        "selected_columns = ['model_0', 'model_1', 'certified', 'fuel_type_from_vin_CNG',\n",
        "                    'fuel_type_from_vin_Diesel', 'fuel_type_from_vin_Electric',\n",
        "                    'fuel_type_from_vin_Hybrid',\n",
        "                    'fuel_type_from_vin_PHEV']\n",
        "X = X_train[selected_columns]\n",
        "y = y_train['transmission_from_vin']\n",
        "\n",
        "# Initialize SMOTE with k_neighbors=1\n",
        "smote = SMOTE(random_state=42, k_neighbors=1) # Changed k_neighbors to 1\n",
        "\n",
        "# Create a copy of X to store resampled data\n",
        "X_resampled = X.copy()\n",
        "\n",
        "# Apply SMOTE to each selected feature\n",
        "for feature in selected_columns:\n",
        "    # Create a temporary target variable for the current feature\n",
        "    temp_y = X[feature]\n",
        "\n",
        "    # Check if the minority class has at least k_neighbors + 1 samples\n",
        "    # If not, skip SMOTE for this feature\n",
        "    unique_values, counts = np.unique(temp_y, return_counts=True)\n",
        "    minority_class_count = counts.min()\n",
        "\n",
        "    if minority_class_count > smote.k_neighbors:  # Check if minority class has enough samples\n",
        "        # Apply SMOTE to the feature and temporary target\n",
        "        X_feature_resampled, _ = smote.fit_resample(X, temp_y)\n",
        "\n",
        "        # Update the resampled data with the balanced feature values\n",
        "        X_resampled[feature] = X_feature_resampled[feature]\n",
        "    else:\n",
        "        print(f\"Skipping SMOTE for feature '{feature}' due to insufficient minority class samples.\")\n",
        "\n",
        "# Apply SMOTE to balance the target variable\n",
        "X_resampled, y_resampled = smote.fit_resample(X_resampled, y)\n",
        "\n",
        "\n",
        "# Recombine with the remaining columns\n",
        "X_train_resampled_combined = pd.concat([X_train.drop(columns=selected_columns), X_resampled], axis=1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QrkMaq92UP63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfo3eIwPdCxP"
      },
      "outputs": [],
      "source": [
        "print(X_train_resampled_combined.shape)\n",
        "print(y_resampled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WVIfr-LZP4i"
      },
      "outputs": [],
      "source": [
        "#count the number of classes in each column of the Train set after handling imbalance\n",
        "print(X_train_resampled_combined['model_0'].value_counts())\n",
        "print(X_train_resampled_combined['model_1'].value_counts())\n",
        "print(X_train_resampled_combined['certified'].value_counts())\n",
        "print(X_train_resampled_combined['fuel_type_from_vin_CNG'].value_counts())\n",
        "print(X_train_resampled_combined['fuel_type_from_vin_Diesel'].value_counts())\n",
        "print(X_train_resampled_combined['fuel_type_from_vin_Electric'].value_counts())\n",
        "print(X_train_resampled_combined['fuel_type_from_vin_Hybrid'].value_counts())\n",
        "print(X_train_resampled_combined['fuel_type_from_vin_Hydrogen'].value_counts())\n",
        "print(X_train_resampled_combined['fuel_type_from_vin_PHEV'].value_counts())\n",
        "print(y_resampled.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ObK6VzRlP18"
      },
      "outputs": [],
      "source": [
        "# Create individual countplots for each column\n",
        "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "\n",
        "sns.countplot(x='model_0', hue = 'model_0', data=X_train_resampled_combined, ax=axes[0, 0])\n",
        "sns.countplot(x='model_1', hue = 'model_1', data=X_train_resampled_combined, ax=axes[0, 1])\n",
        "sns.countplot(x='certified', hue = 'certified', data=X_train_resampled_combined, ax=axes[1, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_CNG', hue = 'fuel_type_from_vin_CNG', data=X_train_resampled_combined, ax=axes[1, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Diesel', hue = 'fuel_type_from_vin_Diesel', data=X_train_resampled_combined, ax=axes[2, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Electric', hue = 'fuel_type_from_vin_Electric', data=X_train_resampled_combined, ax=axes[2, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Hybrid', hue = 'fuel_type_from_vin_Hybrid', data=X_train_resampled_combined, ax=axes[3, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Hydrogen', hue = 'fuel_type_from_vin_Hydrogen', data=X_train_resampled_combined, ax=axes[3, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_PHEV', hue = 'fuel_type_from_vin_PHEV', data=X_train_resampled_combined, ax=axes[4, 0])\n",
        "# Convert y_resampled to a DataFrame\n",
        "y_resampled_df = y_resampled.to_frame()\n",
        "sns.countplot(x='transmission_from_vin', hue = 'transmission_from_vin', data=y_resampled_df, ax=axes[4, 1])\n",
        "# clear extra subplots to avoid empty plots\n",
        "# axes[4, 1].axis('off')\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeAFlA7_Sf3M"
      },
      "source": [
        "**Using SMOTE technique to handle imbalance in Test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tndHAQ4iG79I"
      },
      "outputs": [],
      "source": [
        "# Create individual countplots for each column\n",
        "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "\n",
        "sns.countplot(x='model_0', hue = 'model_0', data=X_test, ax=axes[0, 0])\n",
        "sns.countplot(x='model_1', hue = 'model_1', data=X_test, ax=axes[0, 1])\n",
        "sns.countplot(x='certified', hue = 'certified', data=X_test, ax=axes[1, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_CNG', hue = 'fuel_type_from_vin_CNG', data=X_test, ax=axes[1, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Diesel', hue = 'fuel_type_from_vin_Diesel', data=X_test, ax=axes[2, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Electric', hue = 'fuel_type_from_vin_Electric', data=X_test, ax=axes[2, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Hybrid', hue = 'fuel_type_from_vin_Hybrid', data=X_test, ax=axes[3, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Hydrogen', hue = 'fuel_type_from_vin_Hydrogen', data=X_test, ax=axes[3, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_PHEV', hue = 'fuel_type_from_vin_PHEV', data=X_test, ax=axes[4, 0])\n",
        "sns.countplot(x='transmission_from_vin', hue = 'transmission_from_vin', data=y_test, ax=axes[4, 1])\n",
        "# clear extra subplots to avoid empty plots\n",
        "# axes[4, 1].axis('off')\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xALzlNc19SW"
      },
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "\"\"\"import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Select specific columns (features to balance)\n",
        "selected_columns = ['model_0', 'model_1', 'certified',\n",
        "                    'fuel_type_from_vin_Diesel', 'fuel_type_from_vin_Electric',\n",
        "                    'fuel_type_from_vin_Hybrid',\n",
        "                    'fuel_type_from_vin_PHEV']\n",
        "X1 = X_test[selected_columns]\n",
        "y1 = y_test['transmission_from_vin']\n",
        "\n",
        "# Initialize SMOTE with k_neighbors=1\n",
        "# k_neighbors must be less than or equal to the number of samples in the minority class\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "\n",
        "# Initialize Imputer to replace NaN with most frequent value\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Impute missing values in X1\n",
        "X1_imputed = pd.DataFrame(imputer.fit_transform(X1), columns=X1.columns)\n",
        "\n",
        "# Create a copy of X to store resampled data\n",
        "X1_resampled = X1_imputed.copy()\n",
        "\n",
        "# Apply SMOTE to each selected feature\n",
        "for feature in selected_columns:\n",
        "    # Create a temporary target variable for the current feature\n",
        "    temp_y1 = X1_imputed[feature]\n",
        "\n",
        "    # Check if the minority class has at least k_neighbors + 1 samples\n",
        "    # If not, skip SMOTE for this feature\n",
        "    unique_values, counts = np.unique(temp_y1, return_counts=True)\n",
        "    minority_class_count = counts.min()\n",
        "\n",
        "    if minority_class_count > smote.k_neighbors:  # Check if minority class has enough samples\n",
        "        # Apply SMOTE to the feature and temporary target\n",
        "        X1_feature_resampled, _ = smote.fit_resample(X1_imputed, temp_y1)\n",
        "\n",
        "        # Update the resampled data with the balanced feature values\n",
        "        X1_resampled[feature] = X1_feature_resampled[feature]\n",
        "    else:\n",
        "        print(f\"Skipping SMOTE for feature '{feature}' due to insufficient minority class samples.\")\n",
        "\n",
        "\n",
        "# Apply SMOTE to balance the target variable\n",
        "X1_resampled, y1_resampled = smote.fit_resample(X1_resampled, y1)\n",
        "\n",
        "\n",
        "# Recombine with the remaining columns\n",
        "X_test_resampled_combined = pd.concat([X_test.drop(columns=selected_columns), X1_resampled], axis=1)\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "56Iv6MduaqCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_resampled_combined.shape)\n",
        "print(y1_resampled.shape)"
      ],
      "metadata": {
        "id": "etEw7EnhawHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Select specific columns (features to balance)\n",
        "selected_columns = ['model_0', 'model_1', 'certified',\n",
        "                    'fuel_type_from_vin_Diesel', 'fuel_type_from_vin_Electric',\n",
        "                    'fuel_type_from_vin_Hybrid',\n",
        "                    'fuel_type_from_vin_PHEV']\n",
        "X1 = X_test[selected_columns]\n",
        "y1 = y_test['transmission_from_vin']\n",
        "\n",
        "# Initialize SMOTE with k_neighbors=1\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "\n",
        "# Initialize Imputer to replace NaN with most frequent value\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Impute missing values in X1\n",
        "X1_imputed = pd.DataFrame(imputer.fit_transform(X1), columns=X1.columns)\n",
        "\n",
        "# Create a copy of X to store resampled data\n",
        "X1_resampled = X1_imputed.copy()\n",
        "\n",
        "# Apply SMOTE to each selected feature, but skip if insufficient minority samples\n",
        "for feature in selected_columns:\n",
        "    temp_y1 = X1_imputed[feature]\n",
        "    unique_values, counts = np.unique(temp_y1, return_counts=True)\n",
        "    minority_class_count = counts.min()\n",
        "\n",
        "    if minority_class_count > smote.k_neighbors:\n",
        "        X1_feature_resampled, _ = smote.fit_resample(X1_imputed, temp_y1)\n",
        "        X1_resampled[feature] = X1_feature_resampled[feature]\n",
        "    else:\n",
        "        print(f\"Skipping SMOTE for feature '{feature}' due to insufficient minority class samples.\")\n",
        "\n",
        "# Apply SMOTE to balance the target variable\n",
        "X1_resampled, y1_resampled = smote.fit_resample(X1_resampled, y1)\n",
        "\n",
        "X_test_remaining = X_test.drop(columns=selected_columns)\n",
        "X_test_resampled_combined = pd.concat([X_test_remaining, X1_resampled], axis=1)\n",
        "\n",
        "# If rows don't match, adjust X_test_resampled_combined\n",
        "if X_test_resampled_combined.shape[0] != y1_resampled.shape[0]:\n",
        "    num_rows_to_adjust = y1_resampled.shape[0] - X_test_resampled_combined.shape[0]\n",
        "\n",
        "    if num_rows_to_adjust > 0:  # Need to add rows\n",
        "        additional_rows = X_test_remaining.sample(n=num_rows_to_adjust, replace=True, random_state=42)\n",
        "        X_test_remaining = pd.concat([X_test_remaining, additional_rows], ignore_index=True)\n",
        "        X_test_resampled_combined = pd.concat([X_test_remaining, X1_resampled], axis=1)\n",
        "\n",
        "    elif num_rows_to_adjust < 0:  # Need to remove rows\n",
        "        # Remove extra rows from X_test_resampled_combined\n",
        "        X_test_resampled_combined = X_test_resampled_combined.iloc[:y1_resampled.shape[0]]\n",
        "\n",
        "print(f\"Shape of X_test_resampled_combined: {X_test_resampled_combined.shape}\")\n",
        "print(f\"Shape of y1_resampled: {y1_resampled.shape}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ULkJmf27bXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYl5qmGDSzah"
      },
      "outputs": [],
      "source": [
        "# Create individual countplots for each column\n",
        "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "\n",
        "sns.countplot(x='model_0', hue = 'model_0', data=X_test_resampled_combined, ax=axes[0, 0])\n",
        "sns.countplot(x='model_1', hue = 'model_1', data=X_test_resampled_combined, ax=axes[0, 1])\n",
        "sns.countplot(x='certified', hue = 'certified', data=X_test_resampled_combined, ax=axes[1, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_CNG', hue = 'fuel_type_from_vin_CNG', data=X_test_resampled_combined, ax=axes[1, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Diesel', hue = 'fuel_type_from_vin_Diesel', data=X_test_resampled_combined, ax=axes[2, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Electric', hue = 'fuel_type_from_vin_Electric', data=X_test_resampled_combined, ax=axes[2, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_Hybrid', hue = 'fuel_type_from_vin_Hybrid', data=X_test_resampled_combined, ax=axes[3, 0])\n",
        "sns.countplot(x='fuel_type_from_vin_Hydrogen', hue = 'fuel_type_from_vin_Hydrogen', data=X_test_resampled_combined, ax=axes[3, 1])\n",
        "sns.countplot(x='fuel_type_from_vin_PHEV', hue = 'fuel_type_from_vin_PHEV', data=X_test_resampled_combined, ax=axes[4, 0])\n",
        "\n",
        "# Convert y1_resampled to a DataFrame before using it in sns.countplot\n",
        "y1_resampled_df = y1_resampled.to_frame()\n",
        "\n",
        "# Now use the DataFrame in sns.countplot\n",
        "sns.countplot(x='transmission_from_vin', hue='transmission_from_vin', data=y1_resampled_df, ax=axes[4, 1])\n",
        "\n",
        "# clear extra subplots to avoid empty plots\n",
        "# axes[4, 1].axis('off')\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFcD_Txpw6e2"
      },
      "outputs": [],
      "source": [
        "#count the number of classes in each column of the Train set after handling imbalance\n",
        "print(X_test_resampled_combined['model_0'].value_counts())\n",
        "print(X_test_resampled_combined['model_1'].value_counts())\n",
        "print(X_test_resampled_combined['certified'].value_counts())\n",
        "print(X_test_resampled_combined['fuel_type_from_vin_CNG'].value_counts())\n",
        "print(X_test_resampled_combined['fuel_type_from_vin_Diesel'].value_counts())\n",
        "print(X_test_resampled_combined['fuel_type_from_vin_Electric'].value_counts())\n",
        "print(X_test_resampled_combined['fuel_type_from_vin_Hybrid'].value_counts())\n",
        "print(X_test_resampled_combined['fuel_type_from_vin_Hydrogen'].value_counts())\n",
        "print(X_test_resampled_combined['fuel_type_from_vin_PHEV'].value_counts())\n",
        "print(y_resampled.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlte1FPon75Y"
      },
      "source": [
        "## **7. Scaling Test and Train Sets**\n",
        "### The purpose of scaling is to bring all features (variables) into a common range or distribution. This can improve the performance and convergence speed of machine learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0tMoxp-9FUa"
      },
      "source": [
        "### **Scaling X_train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8mXOSzSXWWy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize MinMaxScaler with desired range (default is 0 to 1)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_X_train_resampled_combined = scaler.fit_transform(X_train_resampled_combined)\n",
        "\n",
        "# Convert the result back to a DataFrame\n",
        "scaled_X_train_resampled_combined = pd.DataFrame(scaled_X_train_resampled_combined, columns=X_train_resampled_combined.columns)\n",
        "\n",
        "print(\"Scaled Data:\")\n",
        "scaled_X_train_resampled_combined.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUwi6-32olPX"
      },
      "source": [
        "### **Scaling X_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWdsb70B4Mo_"
      },
      "outputs": [],
      "source": [
        "# Initialize MinMaxScaler with desired range (default is 0 to 1)\n",
        "scaler2 = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_X_test_resampled_combined = scaler2.fit_transform(X_test_resampled_combined)\n",
        "\n",
        "# Get column names from the original DataFrame\n",
        "#columns = X_test.columns\n",
        "\n",
        "# Convert the result back to a DataFrame using the original column names\n",
        "scaled_X_test_resampled_combined = pd.DataFrame(scaled_X_test_resampled_combined, columns=X_test_resampled_combined.columns)\n",
        "\n",
        "print(\"Scaled Data:\")\n",
        "scaled_X_test_resampled_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJgNWwsFnJ1e"
      },
      "outputs": [],
      "source": [
        "# scaling y1_resampled_df\n",
        "scaler3 = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_y1_resampled_df = scaler3.fit_transform(y1_resampled_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling y1_resampled_df\n",
        "scaler4 = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_y_resampled_df = scaler4.fit_transform(y_resampled_df)"
      ],
      "metadata": {
        "id": "XHVZuRa2c06r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGh-fORdbexZ"
      },
      "outputs": [],
      "source": [
        "print(scaled_X_train_resampled_combined.shape)\n",
        "print(scaled_X_test_resampled_combined.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(scaled_y_resampled.shape)\n",
        "print(scaled_y1_resampled_df.shape)"
      ],
      "metadata": {
        "id": "RTTgPu8jc_yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_resampled.shape) # train\n",
        "print(y1_resampled.shape) # test"
      ],
      "metadata": {
        "id": "kAUlI0HrJM1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft-2JQk18Cob"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JEZ8kW48v1Q"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
        "from sklearn.impute import SimpleImputer # Import the SimpleImputer class from the correct module\n",
        "from sklearn.pipeline import Pipeline  # Import Pipeline for creating the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7AziNoe8BmY"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver ='lbfgs',multi_class='auto')))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVC', SVC(gamma='scale')))\n",
        "models.append(('RFC', RandomForestClassifier(n_estimators=100)))\n",
        "models.append(('DTR', DecisionTreeClassifier()))\n",
        "models.append(('XGB',XGBClassifier()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv1cgdJdYd1L"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "names = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHGKLk-fYiEU"
      },
      "outputs": [],
      "source": [
        "# Import necessary classes\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "# Define the models to evaluate\n",
        "models = [('Logistic Regression', LogisticRegression(solver='lbfgs', multi_class='auto')),\n",
        "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
        "    ('Naive Bayes', GaussianNB()),\n",
        "    ('Support Vector Machine', SVC()),\n",
        "    ('Random Forest', RandomForestClassifier()),\n",
        "    ('Decision Tree', DecisionTreeClassifier()),\n",
        "    ('XGB', XGBClassifier())\n",
        "]\n",
        "\n",
        "# Define the number of folds for k-fold cross-validation\n",
        "num_folds = 5\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# Iterate through the models\n",
        "for name, model in models:\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    # Handle NaN values before cross-validation\n",
        "    # This ensures that any NaN values are filled for each fold\n",
        "    imputer = SimpleImputer(strategy='constant', fill_value=0) # Use an imputer to handle NaNs\n",
        "    pipeline = Pipeline([('imputer', imputer), ('model', model)]) # Create a pipeline with imputation and model\n",
        "\n",
        "    # Perform cross-validation using the pipeline\n",
        "    cv_results = cross_val_score(pipeline, scaled_X_train_resampled_combined, y_resampled, cv=kfold, scoring='accuracy')\n",
        "\n",
        "    # Store the results\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "\n",
        "    # Print the mean and standard deviation of the accuracy scores\n",
        "    print(f\"{name}: {cv_results.mean():.4f} ({cv_results.std():.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rb7WERfLIjt"
      },
      "source": [
        "### **Hyperparameter Tuning**\n",
        "Since RandomForestClassifier is the best performing model based on the cross-validation results, we will be be performimg hyperparameter tuning to identify the best hyperparameter for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2IheLTtLOlI"
      },
      "outputs": [],
      "source": [
        "# hyper parameter tuning of random forest regressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTjSaeStLsA1"
      },
      "outputs": [],
      "source": [
        "#Instantiating\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "# Default parameters\n",
        "RF.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTjW0FFwLskO"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'bootstrap': [True, False],\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0UjU6S2MlZo"
      },
      "outputs": [],
      "source": [
        "# Create a Random Forest Regressor\n",
        "rf_regressor = RandomForestClassifier()\n",
        "\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf_regressor,\n",
        "                           param_grid=param_grid, cv=3,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1, verbose=2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOK3F_QNNCnb"
      },
      "outputs": [],
      "source": [
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(scaled_X_train_resampled_combined, y_resampled)\n",
        "\n",
        "#Use the best estimator from grid search\n",
        "best_rf = grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_URL092hgQ7p"
      },
      "outputs": [],
      "source": [
        "best_rf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names from the training data\n",
        "training_feature_names = scaled_X_train_resampled_combined.columns\n",
        "\n",
        "# Ensure the test data has the same feature names and order\n",
        "scaled_X_test_resampled_combined = scaled_X_test_resampled_combined[training_feature_names]\n",
        "\n",
        "# Now, make predictions\n",
        "y_pred = best_rf.predict(scaled_X_test_resampled_combined)"
      ],
      "metadata": {
        "id": "im1eEFdLEiHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M3mk7-acTc9"
      },
      "source": [
        "### **Evaluating Model Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M361SjrIYwfe"
      },
      "outputs": [],
      "source": [
        "# Evaluate the performance of the best model on the test dataset\n",
        "accuracy = accuracy_score(scaled_y1_resampled_df, y_pred)\n",
        "print(f\"Accuracy of the best model on the test dataset: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the best model on the test dataset\n",
        "accuracy = accuracy_score(y1_resampled, y_pred)  # Changed from scaled_y1_resampled_df to y1_resampled\n",
        "print(f\"Accuracy of the best model on the test dataset: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "jIOLJlLUJXam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y3BuhG3n2I3"
      },
      "outputs": [],
      "source": [
        "# Generate classification report and confusion matrix\n",
        "print(classification_report(scaled_y1_resampled_df, y_pred))\n",
        "cm = confusion_matrix(scaled_y1_resampled_df, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7vSwziMiP3W"
      },
      "outputs": [],
      "source": [
        "# You can also visualize the confusion matrix using ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOt6Lb10pZeN"
      },
      "source": [
        "**Exporting Model Predictions to CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwYWK948pXh0"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame\n",
        "model_df = pd.DataFrame({\n",
        "    'Predictions': y_pred\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "model_df.to_csv('predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Example: Assume you have the following variables:\n",
        "# X_train, y_train - Your feature matrix and target labels\n",
        "# X_test, y_test - Your test feature matrix and target labels\n",
        "\n",
        "# Train a model (for example, a RandomForestClassifier)\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(scaled_X_train_resampled_combined, scaled_y_resampled_df)\n",
        "\n",
        "# Predict probabilities (for ROC curve, we need probabilities, not just predictions)\n",
        "y_pred_prob = model.predict_proba(scaled_X_test_resampled_combined)[:, 1]  # Get the probability for the positive class\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(scaled_y1_resampled_df, y_pred_prob)\n",
        "\n",
        "# Compute AUC score\n",
        "auc_score = roc_auc_score(scaled_y1_resampled_df, y_pred_prob)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc_score:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (no discrimination)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P0KjZUI4ehg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Replace these with your actual model, features, and predictions\n",
        "model = ...  # Trained machine learning model\n",
        "encoded_features = ...  # Pandas DataFrame containing encoded features\n",
        "predictions = ...  # Predictions as a pandas Series or numpy array\n",
        "\n",
        "# File paths\n",
        "model_pkl_path = \"model.pkl\"\n",
        "features_csv_path = \"encoded_features.csv\"\n",
        "predictions_csv_path = \"predictions.csv\"\n",
        "\n",
        "# Save the model as a pickle file\n",
        "with open(model_pkl_path, \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "print(f\"Model saved to {model_pkl_path}\")\n",
        "\n",
        "# Save the encoded features to a CSV file\n",
        "encoded_features.to_csv(features_csv_path, index=False)\n",
        "print(f\"Encoded features saved to {features_csv_path}\")\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "predictions_df = pd.DataFrame(predictions, columns=[\"Prediction\"])\n",
        "predictions_df.to_csv(predictions_csv_path, index=False)\n",
        "print(f\"Predictions saved to {predictions_csv_path}\")\n"
      ],
      "metadata": {
        "id": "yr2ZZnscS5Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he8qY3Bzcmap"
      },
      "source": [
        "#                           **THE END**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4ANqlDDqVpu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}